{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonluffman/Downloads/text_summarization/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 1.91M/1.91M [00:01<00:00, 1.25MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 12.4kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 87.0/87.0 [00:00<00:00, 21.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 295kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 2.28G/2.28G [37:45<00:00, 1.00MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 259/259 [00:00<00:00, 9.62kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Born in 1732 into a Virginia planter family, he learned the morals, manners, and body of knowledge requisite for an 18th century Virginia gentleman.\n",
    "\n",
    "He pursued two intertwined interests: military arts and western expansion. At 16 he helped survey Shenandoah lands for Thomas, Lord Fairfax. Commissioned a lieutenant colonel in 1754, he fought the first skirmishes of what grew into the French and Indian War. The next year, as an aide to Gen. Edward Braddock, he escaped injury although four bullets ripped his coat and two horses were shot from under him.\n",
    "\n",
    "From 1759 to the outbreak of the American Revolution, Washington managed his lands around Mount Vernon and served in the Virginia House of Burgesses. Married to a widow, Martha Dandridge Custis, he devoted himself to a busy and happy life. But like his fellow planters, Washington felt himself exploited by British merchants and hampered by British regulations. As the quarrel with the mother country grew acute, he moderately but firmly voiced his resistance to the restrictions.\n",
    "\n",
    "When the Second Continental Congress assembled in Philadelphia in May 1775, Washington, one of the Virginia delegates, was elected Commander in Chief of the Continental Army. On July 3, 1775, at Cambridge, Massachusetts, he took command of his ill-trained troops and embarked upon a war that was to last six grueling years.\n",
    "\n",
    "He realized early that the best strategy was to harass the British. He reported to Congress, “we should on all Occasions avoid a general Action, or put anything to the Risque, unless compelled by a necessity, into which we ought never to be drawn.” Ensuing battles saw him fall back slowly, then strike unexpectedly. Finally in 1781 with the aid of French allies–he forced the surrender of Cornwallis at Yorktown.\n",
    "\n",
    "Washington longed to retire to his fields at Mount Vernon. But he soon realized that the Nation under its Articles of Confederation was not functioning well, so he became a prime mover in the steps leading to the Constitutional Convention at Philadelphia in 1787. When the new Constitution was ratified, the Electoral College unanimously elected Washington President.\n",
    "\n",
    "He did not infringe upon the policy making powers that he felt the Constitution gave Congress. But the determination of foreign policy became preponderantly a Presidential concern. When the French Revolution led to a major war between France and England, Washington refused to accept entirely the recommendations of either his Secretary of State Thomas Jefferson, who was pro-French, or his Secretary of the Treasury Alexander Hamilton, who was pro-British. Rather, he insisted upon a neutral course until the United States could grow stronger.\n",
    "\n",
    "To his disappointment, two parties were developing by the end of his first term. Wearied of politics, feeling old, he retired at the end of his second. In his Farewell Address, he urged his countrymen to forswear excessive party spirit and geographical distinctions. In foreign affairs, he warned against long-term alliances.\n",
    "\n",
    "Washington enjoyed less than three years of retirement at Mount Vernon, for he died of a throat infection December 14, 1799. For months the Nation mourned him.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[10319,   115,  1689,  6695,   190,   114,  3353, 29440,   328,   108,\n",
       "           178,  1800,   109, 41839,   108, 26611,   108,   111,   513,   113,\n",
       "           825, 25223,   118,   142,  1204,   307,  1902,  3353, 17373,   107,\n",
       "           285, 15739,   228, 32539,  2656,   151,  2002,  3146,   111,  4874,\n",
       "          3847,   107,   654,  1195,   178,  1543,  2629, 43263,  8872,   118,\n",
       "          3203,   108,  2346, 25772,   107, 72511,   114, 36751, 65390,   115,\n",
       "         17382, 11764,   178,  7807,   109,   211, 90528,   113,   180,  3139,\n",
       "           190,   109,  1775,   111,  2128,  1981,   107,   139,   352,   232,\n",
       "           108,   130,   142, 23098,   112,  6888,   107,  7535, 90142,   108,\n",
       "           178, 15680,  2015,  1670,   541, 21841, 16508,   169,  4316,   111,\n",
       "           228,  5525,   195,  1785,   135,   365,   342,   107,  1078,   305,\n",
       "         57699,   112,   109, 16015,   113,   109,   655,  9367,   108,  1741,\n",
       "          2079,   169,  8872,   279,  4636, 18632,   111,  1502,   115,   109,\n",
       "          3353,  1087,   113, 39423,   772,   107, 33154,   112,   114, 20343,\n",
       "           108, 13460,   714,   526, 12239, 80588,  1659,   108,   178,  6774,\n",
       "          1847,   112,   114,  2117,   111,   774,   271,   107,   343,   172,\n",
       "           169,  3345, 28397,   108,  1741,  1373,  1847, 23268,   141,  1816,\n",
       "         12320,   111, 39767,   141,  1816,  3158,   107,   398,   109, 44305,\n",
       "           122,   109,  1499,   531,  3139,  9000,   108,   178, 21472,   155,\n",
       "          8380, 25412,   169,  3497,   112,   109,  5434,   107,   434,   109,\n",
       "          4317, 16235,  3108,  8974,   115,  5546,   115,   913, 75078,   108,\n",
       "          1741,   108,   156,   113,   109,  3353, 12755,   108,   140,  4863,\n",
       "         17247,   115,  3670,   113,   109, 16235,  4136,   107,   651,  1307,\n",
       "           296,   108, 75078,   108,   134,  6631,   108,  5801,   108,   178,\n",
       "           635,  3491,   113,   169,  5313,   121, 14787,  7736,   111, 22958,\n",
       "          1071,   114,  1795,   120,   140,   112,   289,  1029, 44624,   231,\n",
       "           107,   285,  3577,   616,   120,   109,   229,  1520,   140,   112,\n",
       "         44866,   109,  1816,   107,   285,  1668,   112,  3108,   108,   185,\n",
       "          2625,   246,   124,   149, 37623,   116,  1405,   114,   956,  5208,\n",
       "           108,   132,   414,   742,   112,   109, 34662,  7215,   108,  2424,\n",
       "         18702,   141,   114,  9075,   108,   190,   162,   145,  5556,   394,\n",
       "           112,   129,  4188,  1181,  5522, 10777,   273, 10931,  1148,   342,\n",
       "          1251,   247,  3642,   108,   237,  5100, 18239,   107,  4584,   115,\n",
       "         87399,   122,   109,  2637,   113,  1775, 11846,  1198,  3509,  3354,\n",
       "           109, 16553,   113, 16059,  1659,   134, 68575,   107,  1741, 55913,\n",
       "           112, 11807,   112,   169,  2574,   134,  4636, 18632,   107,   343,\n",
       "           178,   783,  3577,   120,   109,  8734,   365,   203, 16643,   113,\n",
       "         37030,   140,   146,  7233,   210,   108,   167,   178,  1257,   114,\n",
       "          4561, 33357,   115,   109,  1550,   964,   112,   109, 26025,  7417,\n",
       "           134,  5546,   115, 78890,   107,   434,   109,   177,  9152,   140,\n",
       "         41948,   108,   109, 34636,  1369, 22264,  4863,  1741,  1276,   107,\n",
       "           285,   368,   146, 32877,  1071,   109,  1067,   395,  6135,   120,\n",
       "           178,  1373,   109,  9152,  1422,  3108,   107,   343,   109,  6796,\n",
       "           113,  2328,  1067,  1257,  1133, 84474, 47604,   114, 16280,  2991,\n",
       "           107,   434,   109,  1775,  9367,  1358,   112,   114,   698,  1795,\n",
       "           317,  2481,   111,  2159,   108,  1741,  6914,   112,  2217,  3143,\n",
       "           109,  3630,   113,   707,   169,  4754,   113,   816,  3203, 11276,\n",
       "           108,   170,   140,  2717,   121, 20295,   108,   132,   169,  4754,\n",
       "           113,   109, 12596,  7414,  7734,   108,   170,   140,  2717,   121,\n",
       "         21372,   107,  7430,   108,   178, 13733,  1071,   114,  6351,   422,\n",
       "           430,   109,   706,  1013,   256,  1248,  4085,   107,   413,   169,\n",
       "         11313,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.generate(**tokens, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[10319,   115,  1689,  6695,   190,   114,  3353, 29440,   328,   108,\n",
       "            178,  1800,   109, 41839,   108, 26611,   108,   111,   513,   113,\n",
       "            825, 25223,   118,   142,  1204,   307,  1902,  3353, 17373,   107,\n",
       "            285, 15739,   228, 32539,  2656,   151,  2002,  3146,   111,  4874,\n",
       "           3847,   107,   654,  1195,   178,  1543,  2629, 43263,  8872,   118,\n",
       "           3203,   108,  2346, 25772,   107, 72511,   114, 36751, 65390,   115,\n",
       "          17382, 11764,   178,  7807,   109,   211, 90528,   113,   180,  3139,\n",
       "            190,   109,  1775,   111,  2128,  1981,   107,   139,   352,   232,\n",
       "            108,   130,   142, 23098,   112,  6888,   107,  7535, 90142,   108,\n",
       "            178, 15680,  2015,  1670,   541, 21841, 16508,   169,  4316,   111,\n",
       "            228,  5525,   195,  1785,   135,   365,   342,   107,  1078,   305,\n",
       "          57699,   112,   109, 16015,   113,   109,   655,  9367,   108,  1741,\n",
       "           2079,   169,  8872,   279,  4636, 18632,   111,  1502,   115,   109,\n",
       "           3353,  1087,   113, 39423,   772,   107, 33154,   112,   114, 20343,\n",
       "            108, 13460,   714,   526, 12239, 80588,  1659,   108,   178,  6774,\n",
       "           1847,   112,   114,  2117,   111,   774,   271,   107,   343,   172,\n",
       "            169,  3345, 28397,   108,  1741,  1373,  1847, 23268,   141,  1816,\n",
       "          12320,   111, 39767,   141,  1816,  3158,   107,   398,   109, 44305,\n",
       "            122,   109,  1499,   531,  3139,  9000,   108,   178, 21472,   155,\n",
       "           8380, 25412,   169,  3497,   112,   109,  5434,   107,   434,   109,\n",
       "           4317, 16235,  3108,  8974,   115,  5546,   115,   913, 75078,   108,\n",
       "           1741,   108,   156,   113,   109,  3353, 12755,   108,   140,  4863,\n",
       "          17247,   115,  3670,   113,   109, 16235,  4136,   107,   651,  1307,\n",
       "            296,   108, 75078,   108,   134,  6631,   108,  5801,   108,   178,\n",
       "            635,  3491,   113,   169,  5313,   121, 14787,  7736,   111, 22958,\n",
       "           1071,   114,  1795,   120,   140,   112,   289,  1029, 44624,   231,\n",
       "            107,   285,  3577,   616,   120,   109,   229,  1520,   140,   112,\n",
       "          44866,   109,  1816,   107,   285,  1668,   112,  3108,   108,   185,\n",
       "           2625,   246,   124,   149, 37623,   116,  1405,   114,   956,  5208,\n",
       "            108,   132,   414,   742,   112,   109, 34662,  7215,   108,  2424,\n",
       "          18702,   141,   114,  9075,   108,   190,   162,   145,  5556,   394,\n",
       "            112,   129,  4188,  1181,  5522, 10777,   273, 10931,  1148,   342,\n",
       "           1251,   247,  3642,   108,   237,  5100, 18239,   107,  4584,   115,\n",
       "          87399,   122,   109,  2637,   113,  1775, 11846,  1198,  3509,  3354,\n",
       "            109, 16553,   113, 16059,  1659,   134, 68575,   107,  1741, 55913,\n",
       "            112, 11807,   112,   169,  2574,   134,  4636, 18632,   107,   343,\n",
       "            178,   783,  3577,   120,   109,  8734,   365,   203, 16643,   113,\n",
       "          37030,   140,   146,  7233,   210,   108,   167,   178,  1257,   114,\n",
       "           4561, 33357,   115,   109,  1550,   964,   112,   109, 26025,  7417,\n",
       "            134,  5546,   115, 78890,   107,   434,   109,   177,  9152,   140,\n",
       "          41948,   108,   109, 34636,  1369, 22264,  4863,  1741,  1276,   107,\n",
       "            285,   368,   146, 32877,  1071,   109,  1067,   395,  6135,   120,\n",
       "            178,  1373,   109,  9152,  1422,  3108,   107,   343,   109,  6796,\n",
       "            113,  2328,  1067,  1257,  1133, 84474, 47604,   114, 16280,  2991,\n",
       "            107,   434,   109,  1775,  9367,  1358,   112,   114,   698,  1795,\n",
       "            317,  2481,   111,  2159,   108,  1741,  6914,   112,  2217,  3143,\n",
       "            109,  3630,   113,   707,   169,  4754,   113,   816,  3203, 11276,\n",
       "            108,   170,   140,  2717,   121, 20295,   108,   132,   169,  4754,\n",
       "            113,   109, 12596,  7414,  7734,   108,   170,   140,  2717,   121,\n",
       "          21372,   107,  7430,   108,   178, 13733,  1071,   114,  6351,   422,\n",
       "            430,   109,   706,  1013,   256,  1248,  4085,   107,   413,   169,\n",
       "          11313,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{**tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 8087,  107,    1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>George Washington was the first President of the United States.</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
